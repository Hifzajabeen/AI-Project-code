{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hifzajabeen/AI-project-code/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aQQUIATLZgD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K_3WXWELkvB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-v2HsdtLy5E",
        "outputId": "9505a89a-5cda-42d6-c795-3f10aa9740f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "\n",
            "The Last Line:  first to get up and stretch out her young body.\n"
          ]
        }
      ],
      "source": [
        "file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = []\n",
        "\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "    \n",
        "print(\"The First Line: \", lines[0])\n",
        "print(\"The Last Line: \", lines[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "JqhAQs5GMFSD",
        "outputId": "590a5af2-784e-41a5-ba78-7fb73e993a69"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:360]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "GJ37rw_xMM1Y",
        "outputId": "4d645345-5bba-4878-9921-9d2d365e3b8c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
        "new_data = data.translate(translator)\n",
        "\n",
        "new_data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "bT41VSuDMVKz",
        "outputId": "33d3ada8-6b5f-4551-eee9-b31ea39556d1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)\n",
        "data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3vAbbgPMawh",
        "outputId": "de97c75b-3fa9-40c9-bb6b-c2d500222c0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function.\n",
        "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj-ILuBPMllb",
        "outputId": "8ec63d6c-c2b6-4800-9d17-6dda2575ca9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2617\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STaDwfTnMuiV",
        "outputId": "f8890a62-2451-46cd-8554-de0a0a485b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Length of sequences are:  3889\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 17,  53],\n",
              "       [ 53, 293],\n",
              "       [293,   2],\n",
              "       [  2,  18],\n",
              "       [ 18, 729],\n",
              "       [729, 135],\n",
              "       [135, 730],\n",
              "       [730, 294],\n",
              "       [294,   8],\n",
              "       [  8, 731]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMBdbCmUMy92"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOA3-SjFM3pW",
        "outputId": "a24b2cd0-9d34-4f26-8105-e2e6adde4acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Data is:  [ 17  53 293   2  18]\n",
            "The responses are:  [ 53 293   2  18 729]\n"
          ]
        }
      ],
      "source": [
        "print(\"The Data is: \", X[:5])\n",
        "print(\"The responses are: \", y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2Z6zhtkM62B",
        "outputId": "37a98289-dc38-49b4-c0ce-77a9b23762d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj5UdT3XM_bQ"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geQG2g7KNEU2",
        "outputId": "5d322efc-1513-41bd-849c-d08586f3f3eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             26170     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2617)              2619617   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,694,787\n",
            "Trainable params: 15,694,787\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "UNe7XRBUNJqp",
        "outputId": "6b1dd89b-bb38-4051-c98e-54c54f6b2a44"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAIjCAYAAADC5+TxAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3de1RU570+8GfPBeaiDAgISQAVjCJekqPRGKLGaM3RJLVRUFHx1trj5eRYY1SS6LGutiamaqBNNVlGj805zcJBSDWml2NPNMRETY0hmqh4I96KCCqCMigDfn9/5Me0U26DvMwM8nzWmj945539fvfeMw/7MrO3JiICIiIFdL4ugIjuHQwUIlKGgUJEyjBQiEgZwz837N+/H2+88YYvaiGiNmTRokV47LHH3NrqbKFcuHAB2dnZXiuKqNaBAwdw4MABX5dBHsjOzsaFCxfqtNfZQqm1bdu2Vi2I6J9NmDABAN97bYGmafW28xgKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlLGrwNl4MCB0Ov1ePjhh5VPe/bs2ejYsSM0TcNXX33V7H5//OMfYbPZsHPnTuW1NZc/1eJNBw4cQK9evaDT6aBpGiIiIvCLX/zC12W5ycnJQWxsLDRNg6ZpiIyMRGpqqq/LajV+HSgHDx7Ek08+2SrT3rRpE95555277udPdx/xp1q8afDgwTh+/DieeuopAMCJEyewfPlyH1flLikpCQUFBYiLi4PNZkNRURF+97vf+bqsVtPgBZb8SUMXc/GlZ555BmVlZb4uA4B/1VJZWYmRI0di3759vi7FJ9r7/Pv1Fkoto9HYKtP1NKi8EWgigm3btmHjxo2tPlZr2rx5M4qLi31dhs+09/lXEig1NTVYsWIFYmJiYDab0a9fP9jtdgBARkYGrFYrdDodBgwYgIiICBiNRlitVvTv3x9Dhw5FdHQ0TCYTgoODsXTp0jrTP336NOLj42G1WmE2mzF06FB8+umnHtcAfPeBXbNmDXr27InAwEDYbDYsWbKkzlie9Pv0008RExMDTdPwm9/8BgCwYcMGWK1WWCwW7NixA2PGjEFQUBCioqKQmZlZp9ZXX30VPXv2hNlsRlhYGLp164ZXX30VEydObNayb0ktv/71r2EymdC5c2fMnTsX9913H0wmExITE/H555+7+i1YsAABAQGIjIx0tf37v/87rFYrNE3DlStXAAALFy7Eiy++iDNnzkDTNHTv3r1Z86JKW5//vXv3IiEhATabDSaTCX379sX//u//AvjumF7t8Zi4uDjk5eUBAGbNmgWLxQKbzYYPPvgAQOOfiV/+8pewWCzo2LEjiouL8eKLL+KBBx7AiRMn7qpmF/kndrtd6mlu1OLFiyUwMFCys7OltLRUXnnlFdHpdHLw4EEREfnpT38qAOTzzz+XiooKuXLliowePVoAyB/+8AcpKSmRiooKWbBggQCQr776yjXtkSNHSmxsrHz77bfidDrlm2++kUcffVRMJpOcPHnS4xqWLVsmmqbJunXrpLS0VBwOh6xfv14ASF5enms6nva7cOGCAJA333zT7bUA5KOPPpKysjIpLi6WoUOHitVqlaqqKle/VatWiV6vlx07dojD4ZBDhw5JRESEDB8+vFnLXUUtc+bMEavVKseOHZNbt27J0aNHZeDAgdKxY0c5f/68q9/UqVMlIiLCbdw1a9YIACkpKXG1JSUlSVxc3F3NR3JysiQnJzf7df/6r/8qAKS0tNTV5m/zHxcXJzabzaP52bZtm6xcuVKuXbsmV69elcGDB0toaKjbGHq9Xv72t7+5vW7KlCnywQcfuP725DMBQH7yk5/Im2++KePHj5fjx497VCMAsdvtddpbvIVy69YtbNiwAePGjUNSUhKCg4OxfPlyGI1GbNmyxa1vQkICLBYLQkNDMXnyZABATEwMwsLCYLFYXEe/8/Pz3V7XsWNHdO3aFQaDAb1798Y777yDW7duuXYPmqqhsrIS6enp+N73vodFixYhODgYZrMZnTp1chvH035NSUxMRFBQEMLDw5GSkoKKigqcP3/e9fz27dsxYMAAjB07FmazGf3798cPfvADfPLJJ6iqqmrWWC2tBQAMBgN69eqFwMBAJCQkYMOGDbhx40ad9dcWtcX5T05Oxk9/+lOEhISgU6dOGDt2LK5evYqSkhIAwLx581BTU+NWX3l5OQ4ePIinn34aQPM+l6tXr8bzzz+PnJwcxMfHt6j2FgfKiRMn4HA40KdPH1eb2WxGZGRknWD4RwEBAQCA6upqV1vtsRKn09nomH379oXNZsORI0c8quH06dNwOBwYOXJko9P1tF9z1M7nP87TrVu36pyZqampgdFohF6vVza2J7XU55FHHoHFYml0/bVFbXX+az8XNTU1AIARI0agR48e+K//+i/X+2jr1q1ISUlxvX/u9nPZUi0OlIqKCgDA8uXLXft2mqbh3LlzcDgcLS6wIUaj0fXGaKqGixcvAgDCw8Mbnaan/Vrq6aefxqFDh7Bjxw5UVlbiiy++wPbt2/Hss8+2aqA0R2BgoOs/Ynvky/n/wx/+gOHDhyM8PByBgYF1jitqmoa5c+eioKAAH330EQDgv//7v/GjH/3I1cdXn8sWB0rthy89PR0i4vbYv39/iwusT3V1Na5du4aYmBiPajCZTACA27dvNzpdT/u11MqVKzFixAjMnDkTQUFBGD9+PCZOnOjR92K8wel04vr164iKivJ1KT7h7fn/5JNPkJ6eDgA4f/48xo0bh8jISHz++ecoKyvD66+/Xuc1M2fOhMlkwqZNm3DixAkEBQWhS5curud98bkEFHwPpfYMTWPfNlVtz549uHPnDvr37+9RDX369IFOp0Nubi7mzZvX4HQ97ddSR48exZkzZ1BSUgKDwf++CvTxxx9DRDB48GBXm8FgaHJX4V7h7fk/dOgQrFYrAODrr7+G0+nE/PnzERsbC6D+ry2EhIRg0qRJ2Lp1Kzp27Igf//jHbs/74nMJKNhCMZlMmDVrFjIzM7FhwwaUl5ejpqYGFy9exKVLl1TUiKqqKpSVlaG6uhpffvklFixYgC5dumDmzJke1RAeHo6kpCRkZ2dj8+bNKC8vx5EjR+p858PTfi31/PPPIyYmBjdv3lQ63bt1584dlJaWorq6GkeOHMHChQsRExPjWr4A0L17d1y7dg3bt2+H0+lESUkJzp07V2danTp1QmFhIc6ePYsbN260iRDy1fw7nU5cvnwZH3/8sStQare6/+///g+3bt3CqVOn3E5h/6N58+bh9u3b+PDDD/H973/f7TlvfC7r9c+nfe7mtPHt27clLS1NYmJixGAwSHh4uCQlJcnRo0clIyNDLBaLAJCuXbvK3r17ZfXq1WKz2QSAREREyHvvvSdbt26ViIgIASAhISGSmZkpIiJbtmyRJ598Ujp37iwGg0FCQ0Nl8uTJcu7cOY9rEBG5ceOGzJ49W0JDQ6VDhw4yZMgQWbFihQCQqKgoOXz4sMf93nzzTYmMjBQAYrFYZOzYsbJ+/XrXfD744INy5swZ2bhxowQFBQkA6dKli+s09+7duyU0NFQAuB5Go1F69eolOTk5zVr2La1lzpw5YjQa5YEHHhCDwSBBQUHy3HPPyZkzZ9zGuXr1qjz55JNiMpmkW7du8h//8R+yZMkSASDdu3d3nWL98ssvpUuXLmI2m2XIkCFSVFTk8bw097TxgQMHpHfv3qLT6QSAREZGyqpVq/xq/t966y2Ji4tzW9f1Pd5//33XWGlpadKpUycJDg6WCRMmyG9+8xsBIHFxcW6nskVE/uVf/kVefvnlepdPY5+J119/XcxmswCQ6Oho+Z//+R+Pl7tIw6eNlQQKNc/69etl4cKFbm23b9+WF154QQIDA8XhcHitljlz5kinTp28Nl5j7vZ7KC3hT/N/N55++mkpKCjw+rgNBYr/7cDf44qKirBgwYI6+7YBAQGIiYmB0+mE0+mE2Wz2Wk21pyPbq7Y0/06n03Ua+ciRIzCZTOjWrZuPq/q7NvFbnnuJ2WyG0WjE5s2bcfnyZTidThQWFmLTpk1YsWIFUlJSUFhY6Haqr6FHSkqKr2eHvCwtLQ2nTp3CyZMnMWvWLPz85z/3dUluGCheZrPZsGvXLnzzzTfo0aMHzGYzEhISsGXLFqxevRrvvvsu4uPj65zqq++xdevWFtXyyiuvYMuWLSgrK0O3bt2QnZ2taC7bhrY4/xaLBfHx8fje976HlStXIiEhwdcludH+//6QS1ZWFiZNmtRur7FBvjNhwgQAwLZt23xcCTVF0zTY7fY6P2blFgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDV5gqfaXn0TecuDAAQB877VldQIlOjoaycnJvqiF/NQXX3wB4LsbYLWmf7zKPPm35ORkREdH12mvcz0Uon9We82LrKwsH1dC/o7HUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBlNRMTXRZD/+O1vf4uMjAzU1NS42kpKSgAA4eHhrja9Xo+FCxdi5syZ3i6R/BgDhdycOHEC8fHxHvU9fvy4x32pfeAuD7np2bMn+vbtC03TGuyjaRr69u3LMKE6GChUx/Tp06HX6xt83mAwYMaMGV6siNoK7vJQHYWFhYiKikJDbw1N03D+/HlERUV5uTLyd9xCoTruv/9+JCYmQqer+/bQ6XRITExkmFC9GChUr2nTptV7HEXTNEyfPt0HFVFbwF0eqte1a9cQERGB6upqt3a9Xo/Lly8jNDTUR5WRP+MWCtWrU6dOGDVqFAwGg6tNr9dj1KhRDBNqEAOFGpSamoo7d+64/hYRTJs2zYcVkb/jLg81qKKiAmFhYbh16xYAIDAwEFeuXEGHDh18XBn5K26hUIOsVivGjh0Lo9EIg8GA5557jmFCjWKgUKOmTp2K6upq1NTUYMqUKb4uh/ycoeku6uzfvx8XLlzw5pDUQjU1NTCZTBAR3Lx5E1lZWb4uiZohOjoajz32mPcGFC9KTk4WAHzwwYeXHsnJyd78iItXt1AAIDk5Gdu2bfP2sHQXJkyYAACYP38+NE3D8OHDfVsQNUvt+vMmrwcKtT1PPPGEr0ugNoKBQk2q7zc9RPXhO4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyrTbQBk4cCD0ej0efvhh5dOePXs2OnbsCE3T8NVXXzW73x//+EfYbDbs3LlTeW2tKScnB7GxsdA0rcFH165dlYzF9eef2m2gHDx4EE8++WSrTHvTpk1455137rqftNHrhiclJaGgoABxcXGw2WwQEYgIqqur4XA4cPnyZVgsFiVjcf35p3Z/+YL67o7na8888wzKysp8XYYyer0eZrMZZrMZPXr0UDptrj//0m63UGoZjcZWma6nb3RvfCBEBNu2bcPGjRtbfaymbN++Xen0uP78i98HSk1NDVasWIGYmBiYzWb069cPdrsdAJCRkQGr1QqdTocBAwYgIiICRqMRVqsV/fv3x9ChQxEdHQ2TyYTg4GAsXbq0zvRPnz6N+Ph4WK1WmM1mDB06FJ9++qnHNQDfrfA1a9agZ8+eCAwMhM1mw5IlS+qM5Um/Tz/9FDExMdA0Db/5zW8AABs2bIDVaoXFYsGOHTswZswYBAUFISoqCpmZmXVqffXVV9GzZ0+YzWaEhYWhW7duePXVVzFx4sS7WwmthOuvba+/ennzArbJycnNvmju4sWLJTAwULKzs6W0tFReeeUV0el0cvDgQRER+elPfyoA5PPPP5eKigq5cuWKjB49WgDIH/7wBykpKZGKigpZsGCBAJCvvvrKNe2RI0dKbGysfPvtt+J0OuWbb76RRx99VEwmk5w8edLjGpYtWyaapsm6deuktLRUHA6HrF+/XgBIXl6eazqe9rtw4YIAkDfffNPttQDko48+krKyMikuLpahQ4eK1WqVqqoqV79Vq1aJXq+XHTt2iMPhkEOHDklERIQMHz68Wctd5O7Wl4hIXFyc2Gw2t7af/OQn8vXXX9fpy/Xnf+uvJfw6UCorK8VisUhKSoqrzeFwSGBgoMyfP19E/v6GvHHjhqvPu+++KwDc3sB//etfBYBs3brV1TZy5Eh56KGH3MY8cuSIAJDFixd7VIPD4RCLxSKjRo1ym05mZqbbG83TfiKNvyErKytdbbVv5tOnT7vaBg4cKIMGDXIb49/+7d9Ep9PJ7du3pTlaEiio5wrsjQUK1993/GH9tYRf7/KcOHECDocDffr0cbWZzWZERkYiPz+/wdcFBAQAAKqrq11ttfvaTqez0TH79u0Lm82GI0eOeFTD6dOn4XA4MHLkyEan62m/5qidz3+cp1u3btU5y1BTUwOj0Qi9Xq9s7Kb841keEcFPfvITj1/L9ef79Xe3/DpQKioqAADLly93+y7DuXPn4HA4Wm1co9HoWslN1XDx4kUAQHh4eKPT9LRfSz399NM4dOgQduzYgcrKSnzxxRfYvn07nn32WZ++ITMyMtw+1K2J6893/DpQaldeenq62387EcH+/ftbZczq6mpcu3YNMTExHtVgMpkAALdv3250up72a6mVK1dixIgRmDlzJoKCgjB+/HhMnDjRo+9V3Au4/nzLrwOl9gh/Y99WVG3Pnj24c+cO+vfv71ENffr0gU6nQ25ubqPT9bRfSx09ehRnzpxBSUkJnE4nzp8/jw0bNiAkJKRVx/XUpUuXMGvWrFabPtefb/l1oJhMJsyaNQuZmZnYsGEDysvLUVNTg4sXL+LSpUtKxqiqqkJZWRmqq6vx5ZdfYsGCBejSpQtmzpzpUQ3h4eFISkpCdnY2Nm/ejPLychw5cqTOdwY87ddSzz//PGJiYnDz5k2l020pEUFlZSVycnIQFBSkbLpcf37Gm0eA7+ao8+3btyUtLU1iYmLEYDBIeHi4JCUlydGjRyUjI0MsFosAkK5du8revXtl9erVYrPZBIBERETIe++9J1u3bpWIiAgBICEhIZKZmSkiIlu2bJEnn3xSOnfuLAaDQUJDQ2Xy5Mly7tw5j2sQEblx44bMnj1bQkNDpUOHDjJkyBBZsWKFAJCoqCg5fPiwx/3efPNNiYyMFABisVhk7Nixsn79etd8Pvjgg3LmzBnZuHGjBAUFCQDp0qWL6zTp7t27JTQ01O3sitFolF69eklOTk6rrq/333+/wTM8//hYvny5iAjXn5+tPxU0Ee/98KD2Xqu8t3Hr2bBhA06dOoX09HRXW1VVFV566SVs2LABpaWlMJvNHk2L68v72vr6a/e/5bmXFBUVYcGCBXWOFwQEBCAmJgZOpxNOp9PjNyR5172w/vz6GAo1j9lshtFoxObNm3H58mU4nU4UFhZi06ZNWLFiBVJSUpQevyC17oX1x0C5h9hsNuzatQvffPMNevToAbPZjISEBGzZsgWrV6/Gu+++6+sSqRH3wvrjLs89ZujQofjLX/7i6zLoLrX19cctFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImW8/mvjixcvIisry9vD0l2ovXUE11fbdPHiRURFRXl3UG9ebzI5ObnJ643ywQcf6h739DVlqW2qvUk3t1SoKTyGQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyhh8XQD5l9zcXBw4cMCtLT8/HwDw+uuvu7UPHjwYTzzxhNdqI/+niYj4ugjyH3/5y1/w1FNPwWg0QqerfwP2zp07cDqd2LVrF0aNGuXlCsmfMVDITU1NDSIiInD16tVG+4WEhKC4uBgGAzdy6e94DIXc6PV6TJ06FQEBAQ32CQgIwLRp0xgmVAcDheqYPHkyqqqqGny+qqoKkydP9mJF1FZwl4fq1aVLF5w/f77e56KionD+/HlomublqsjfcQuF6pWamgqj0VinPSAgADNmzGCYUL24hUL1On78OBISEup97uuvv0afPn28XBG1BQwUalBCQgKOHz/u1hYfH1+njagWd3moQdOnT3fb7TEajZgxY4YPKyJ/xy0UatD58+fRtWtX1L5FNE1DQUEBunbt6tvCyG9xC4UaFBMTg0ceeQQ6nQ6apmHgwIEME2oUA4UaNX36dOh0Ouj1ekybNs3X5ZCf4y4PNaqkpAT33XcfAOBvf/sbIiIifFwR+TMGShOysrIwadIkX5dBfsBut2PixIm+LsOv8ccYHrLb7b4uQbn09HQAwAsvvNBov9zcXGiahmHDhnmjLL/EfyqeYaB46F78z7Rt2zYATc/b6NGjAQBBQUGtXpO/YqB4hoFCTWrPQULNw7M8RKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQFFu7di06d+4MTdPw9ttv+7ocpXJychAbGwtN06BpGiIjI5Gamtrk6w4fPoyUlBR069YNgYGBCAsLw0MPPYRf/OIXrj4pKSmu6Tb1+PDDD+vU8p//+Z+N1vDGG29A0zTodDrEx8fjk08+afHyoLoYKIotXrwY+/bt83UZrSIpKQkFBQWIi4uDzWZDUVERfve73zX6mq+//hqJiYmIjIzEnj17UFZWhn379mH06NH4+OOP3fru2rUL169fh9PpxKVLlwAAY8eORVVVFSoqKlBcXIwf//jHdWoBgE2bNsHpdNZbQ01NDX79618DAEaMGIH8/Px2fbGo1sRA8QOVlZVITEz0dRmtYu3atQgODkZGRga6du0Kk8mEHj164Oc//znMZrOrn6ZpePzxx2Gz2WAwGNzajUYjLBYLwsPDMWDAgDpjDBgwAEVFRdi+fXu9NeTk5OCBBx5QP3NUBwPFD2zevBnFxcW+LqNVXL16FWVlZbh27Zpbe0BAAHbu3On6OzMzExaLpcnpzZkzB88++6xb2/z58wEAb731Vr2veeONN/Diiy82t3S6CwwUL8nNzcWgQYNgsVgQFBSEvn37ory8HAsXLsSLL76IM2fOQNM0dO/eHRkZGbBardDpdBgwYAAiIiJgNBphtVrRv39/DB06FNHR0TCZTAgODsbSpUt9PXsNGjhwICoqKjBixAh89tlnrTLGiBEj0KtXL+zZswcnTpxwe+6zzz6Dw+HAU0891SpjkzsGihdUVFRg7NixSE5OxrVr13Dq1Cn06NEDVVVVyMjIwPe//33ExcVBRHD69GksXLgQS5YsgYjgrbfewrfffouioiIMGzYMeXl5ePnll5GXl4dr165hxowZWLNmDQ4fPuzr2azX0qVL8cgjj+Dw4cMYMmQIevfujV/+8pd1tlhaau7cuQBQ50D4unXrsGjRIqVjUcMYKF5w9uxZlJeXo3fv3jCZTIiIiEBOTg7CwsKafG1CQgIsFgtCQ0MxefJkAN/d0S8sLAwWi8V1liU/P79V5+Fumc1m7Nu3D7/61a8QHx+PY8eOIS0tDb169UJubq6ycWbMmAGr1Yp3330XlZWVAICCggIcPHgQU6ZMUTYONY6B4gWxsbHo3LkzUlNTsXLlSpw9e/auphMQEAAAqK6udrXV3sy8oTMc/sBoNGLBggU4fvw4Dhw4gOeeew7FxcWYMGECSktLlYxhs9kwZcoUlJaWYuvWrQC+u03I/PnzXcuNWh8DxQvMZjN2796NIUOGYNWqVYiNjUVKSorrP2l78uijj+L3v/895s2bh5KSEuzZs0fZtGsPzr799tu4fv06tm3b5toVIu9goHhJ7969sXPnThQWFiItLQ12ux1r1671dVnKffLJJ64biAHffV/kH7eoatXeJ9nhcCgb++GHH8bgwYPx17/+FXPmzMGECRMQEhKibPrUNAaKFxQWFuLYsWMAgPDwcLz22mvo37+/q+1ecujQIVitVtfft2/frnc+a8/G9OvXT+n4tVsp2dnZTd4RkdRjoHhBYWEh5s6di/z8fFRVVSEvLw/nzp3D4MGDAQCdOnVCYWEhzp49ixs3bvj18ZCGOJ1OXL58GR9//LFboADAuHHjkJWVhevXr6OsrAw7duzASy+9hB/84AfKA2XixIkICwvDuHHjEBsbq3Ta5AGhRtntdmnOYlq3bp1EREQIALFarTJ+/Hg5e/asJCYmSkhIiOj1ern//vtl2bJlUl1dLSIiX375pXTp0kXMZrMMGTJEXn75ZbFYLAJAunbtKnv37pXVq1eLzWYTABIRESHvvfeebN261TVWSEiIZGZmNmvekpOTJTk52eP+77//vsTFxQmARh/vv/++6zW7du2SSZMmSVxcnAQGBkpAQID07NlTVq5cKbdu3aozRnl5uQwbNkw6deokAESn00n37t1l1apVDdYSFhYmzz//vOu5pUuXyr59+1x/L1++XCIjI13TS0hIkL179zZnUQkAsdvtzXpNe6SJiHg9xdqQrKwsTJo0CffiYpowYQKAv9/jmBqmaRrsdvs9eY9rlbjLQ0TKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMoYmu5CwHdX7LpX3cvzRt7FS0A24eLFi9i3b5+vy/Cp2ttitPeryCcmJiIqKsrXZfg1Bgo1qfY6qllZWT6uhEW5gXgAABnPSURBVPwdj6EQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlDH4ugDyL1euXEF5eblbW0VFBQCgoKDArT0oKAhhYWFeq438nyYi4usiyH9s3rwZs2fP9qjvpk2b8KMf/aiVK6K2hIFCbkpLSxEREQGn09loP6PRiMuXLyMkJMRLlVFbwGMo5CYkJASjR4+GwdDw3rDBYMCYMWMYJlQHA4XqSE1NRU1NTYPP19TUIDU11YsVUVvBXR6q49atWwgNDYXD4aj3ebPZjCtXrsBisXi5MvJ33EKhOkwmE8aNGwej0VjnOaPRiKSkJIYJ1YuBQvWaMmVKvQdmnU4npkyZ4oOKqC3gLg/Vq7q6Gp07d0Zpaalbe3BwMIqLi+vdeiHiFgrVy2AwICUlBQEBAa42o9GIKVOmMEyoQQwUatDkyZNRVVXl+tvpdGLy5Mk+rIj8HXd5qEEigqioKBQWFgIAIiMjUVhYCE3TfFwZ+StuoVCDNE1DamoqAgICYDQaMX36dIYJNYqBQo2q3e3h2R3yRLv9tfH+/fvxxhtv+LqMNqFDhw4AgF/84hc+rqRtWLRoER577DFfl+ET7XYL5cKFC8jOzvZ1GW2CTqeDTtdu3yrNkp2djQsXLvi6DJ9pt1sotbZt2+brEvzemDFjAHBZeaK9H2Nq94FCTavd5SFqCrdjiUgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgeGjt2rXo3LkzNE3D22+/7etyPHLnzh2kp6cjMTHRq+Pm5OQgNjYWmqZB0zRERkZ6dOvSw4cPIyUlBd26dUNgYCDCwsLw0EMPuV3YKSUlxTXdph4ffvhhnVr+8z//s9Ea3njjDWiaBp1Oh/j4eHzyySctXh7tCQPFQ4sXL8a+fft8XYbHTp06hWHDhmHRokUN3lK0tSQlJaGgoABxcXGw2WwoKirC7373u0Zf8/XXXyMxMRGRkZHYs2cPysrKsG/fPowePRoff/yxW99du3bh+vXrcDqduHTpEgBg7NixqKqqQkVFBYqLi/HjH/+4Ti0AsGnTpnpvYAZ8d8/mX//61wCAESNGID8/H8OGDWvJomh3GCitqLKy0utbB8B3/+lfeuklzJs3Dw8//LDXx78ba9euRXBwMDIyMtC1a1eYTCb06NEDP//5z2E2m139NE3D448/DpvNBoPB4NZuNBphsVgQHh6OAQMG1BljwIABKCoqwvbt2+utIScnBw888ID6mWtHGCitaPPmzSguLvb6uA899BBycnIwdepUBAYGen38u3H16lWUlZXh2rVrbu0BAQHYuXOn6+/MzEyP7qs8Z84cPPvss25t8+fPBwC89dZb9b7mjTfewIsvvtjc0ukfMFBaKDc3F4MGDYLFYkFQUBD69u2L8vJyLFy4EC+++CLOnDkDTdPQvXt3ZGRkwGq1QqfTYcCAAYiIiIDRaITVakX//v0xdOhQREdHw2QyITg4GEuXLvX17HnNwIEDUVFRgREjRuCzzz5rlTFGjBiBXr16Yc+ePThx4oTbc5999hkcDgeeeuqpVhm7vWCgtEBFRQXGjh2L5ORkXLt2DadOnUKPHj1QVVWFjIwMfP/730dcXBxEBKdPn8bChQuxZMkSiAjeeustfPvttygqKsKwYcOQl5eHl19+GXl5ebh27RpmzJiBNWvW4PDhw76eTa9YunQpHnnkERw+fBhDhgxB79698ctf/rLOFktLzZ07FwDqHFhft24dFi1apHSs9oiB0gJnz55FeXk5evfuDZPJhIiICOTk5CAsLKzJ1yYkJMBisSA0NNR1e8+YmBiEhYXBYrG4zork5+e36jz4C7PZjH379uFXv/oV4uPjcezYMaSlpaFXr17Izc1VNs6MGTNgtVrx7rvvorKyEgBQUFCAgwcP8r5DCjBQWiA2NhadO3dGamoqVq5cibNnz97VdGpvSF5dXe1qq70heUNnJO5FRqMRCxYswPHjx3HgwAE899xzKC4uxoQJE1BaWqpkDJvNhilTpqC0tBRbt24FAKSnp2P+/PluN4anu8NAaQGz2Yzdu3djyJAhWLVqFWJjY5GSkuL6z0d379FHH8Xvf/97zJs3DyUlJdizZ4+yadcenH377bdx/fp1bNu2zbUrRC3DQGmh3r17Y+fOnSgsLERaWhrsdjvWrl3r67L83ieffIL09HTX30lJSW5baLWmTZsGAEq/S/Pwww9j8ODB+Otf/4o5c+ZgwoQJCAkJUTb99oyB0gKFhYU4duwYACA8PByvvfYa+vfv72qjhh06dAhWq9X19+3bt+tdbrVnY/r166d0/NqtlOzsbLzwwgtKp92eMVBaoLCwEHPnzkV+fj6qqqqQl5eHc+fOYfDgwQCATp06obCwEGfPnsWNGzfa1fGQhjidTly+fBkff/yxW6AAwLhx45CVlYXr16+jrKwMO3bswEsvvYQf/OAHygNl4sSJCAsLw7hx4xAbG6t02u2atFN2u12aM/vr1q2TiIgIASBWq1XGjx8vZ8+elcTERAkJCRG9Xi/333+/LFu2TKqrq0VE5Msvv5QuXbqI2WyWIUOGyMsvvywWi0UASNeuXWXv3r2yevVqsdlsAkAiIiLkvffek61bt7rGCgkJkczMzGbN2/79++Xxxx+X++67TwAIAImMjJTExETJzc1t1rRERJKTkyU5Odnj/u+//77ExcW5xm7o8f7777tes2vXLpk0aZLExcVJYGCgBAQESM+ePWXlypVy69atOmOUl5fLsGHDpFOnTgJAdDqddO/eXVatWtVgLWFhYfL888+7nlu6dKns27fP9ffy5cslMjLSNb2EhATZu3dvcxaVABC73d6s19xLNBERL2eYX8jKysKkSZPQTme/WSZMmACA9zb2hKZpsNvtmDhxoq9L8Qnu8hCRMgyUNiA/P9+jn+unpKT4ulRq5wxNdyFfi4+P564ZtQncQiEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyrT7yxfUXo2MGnbgwAEAXFbUtHYbKNHR0UhOTvZ1GW2CwdBu3ybNlpycjOjoaF+X4TPt9pqy5Lna66NmZWX5uBLydzyGQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEymgiIr4ugvzHb3/7W2RkZKCmpsbVVlJSAgAIDw93ten1eixcuBAzZ870donkxxgo5ObEiROIj4/3qO/x48c97kvtA3d5yE3Pnj3Rt29faJrWYB9N09C3b1+GCdXBQKE6pk+fDr1e3+DzBoMBM2bM8GJF1FZwl4fqKCwsRFRUFBp6a2iahvPnzyMqKsrLlZG/4xYK1XH//fcjMTEROl3dt4dOp0NiYiLDhOrFQKF6TZs2rd7jKJqmYfr06T6oiNoC7vJQva5du4aIiAhUV1e7tev1ely+fBmhoaE+qoz8GbdQqF6dOnXCqFGjYDAYXG16vR6jRo1imFCDGCjUoNTUVNy5c8f1t4hg2rRpPqyI/B13eahBFRUVCAsLw61btwAAgYGBuHLlCjp06ODjyshfcQuFGmS1WjF27FgYjUYYDAY899xzDBNqFAOFGjV16lRUV1ejpqYGU6ZM8XU55OcMTXdpn7Kysnxdgl+oqamByWSCiODmzZtcLv/fxIkTfV2CX+IxlAY09lsWIn5s6sddnkbY7XaISLt9JCcnIzk5Gbt378aePXt8Xo8/POx2u6/fln6NuzzUpCeeeMLXJVAbwUChJtX3mx6i+vCdQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGCitZPbs2ejYsSM0TcNXX33l63K8IicnB7GxsdA0ze0REBCAzp07Y/jw4VizZg1KS0t9XSq1EgZKK9m0aRPeeecdX5fhVUlJSSgoKEBcXBxsNhtEBHfu3EFxcTGysrLQrVs3pKWloXfv3vjiiy98XS61AgYKtSpN0xAcHIzhw4djy5YtyMrKwuXLl/HMM8+grKzM1+WRYgyUVsTLSNaVnJyMmTNnori4GG+//bavyyHFGCiKiAjWrFmDnj17IjAwEDabDUuWLKnTr6amBitWrEBMTAzMZjP69evnuqzghg0bYLVaYbFYsGPHDowZMwZBQUGIiopCZmam23Ryc3MxaNAgWCwWBAUFoW/fvigvL29yDH8wc+ZMAMCf/vQnVxuXyz1CqF4AxG63e9x/2bJlommarFu3TkpLS8XhcMj69esFgOTl5bn6LV68WAIDAyU7O1tKS0vllVdeEZ1OJwcPHnRNB4B89NFHUlZWJsXFxTJ06FCxWq1SVVUlIiI3b96UoKAgef3116WyslKKiopk/PjxUlJS4tEYnkpOTpbk5ORmvUZEJC4uTmw2W4PPl5eXCwCJjo52tbWV5WK324Ufm4ZxyTSgOYHicDjEYrHIqFGj3NozMzPdAqWyslIsFoukpKS4vTYwMFDmz58vIn//4FRWVrr61AbT6dOnRUTkm2++EQDy4Ycf1qnFkzE81VqBIiKiaZoEBwd7XLO/LBcGSuO4y6PA6dOn4XA4MHLkyEb7nThxAg6HA3369HG1mc1mREZGIj8/v8HXBQQEAACcTicAIDY2Fp07d0ZqaipWrlyJs2fPtngMb6qoqICIICgoCACXy72EgaLAxYsXAQDh4eGN9quoqAAALF++3O17GufOnYPD4fB4PLPZjN27d2PIkCFYtWoVYmNjkZKSgsrKSmVjtKaTJ08CAOLj4wFwudxLGCgKmEwmAMDt27cb7VcbOOnp6XXu97J///5mjdm7d2/s3LkThYWFSEtLg91ux9q1a5WO0Vr+/Oc/AwDGjBkDgMvlXsJAUaBPnz7Q6XTIzc1ttF90dDRMJlOLvzlbWFiIY8eOAfjuw/jaa6+hf//+OHbsmLIxWktRURHS09MRFRWFH/7whwC4XO4lDBQFwsPDkZSUhOzsbGzevBnl5eU4cuQINm7c6NbPZDJh1qxZyMzMxIYNG1BeXo6amhpcvHgRly5d8ni8wsJCzJ07F/n5+aiqqkJeXh7OnTuHwYMHKxujpUS+uxfynTt3ICIoKSmB3W7H448/Dr1ej+3bt7uOobSn5XLP8/JB4DYDzTxtfOPGDZk9e7aEhoZKhw4dZMiQIbJixQoBIFFRUXL48GEREbl9+7akpaVJTEyMGAwGCQ8Pl6SkJDl69KisX79eLBaLAJAHH3xQzpw5Ixs3bpSgoCABIF26dJGTJ0/K2bNnJTExUUJCQkSv18v9998vy5Ytk+rq6ibHaI7mnuX54IMPpF+/fmKxWCQgIEB0Op0AcJ3RGTRokPzsZz+Tq1ev1nltW1kuPMvTON4svQGapsFut2PixIm+LsVnJkyYAADYtm2bjyvxH1lZWZg0aRL4sakfd3mISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGYOvC/Bn7f1q6LW3B8nKyvJxJf6jvb8nmsJLQDaANzqnxvBjUz9uoTSAb5i/q72uLrdUqCk8hkJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoBCRMgwUIlKGgUJEyjBQiEgZBgoRKcNAISJlGChEpAwDhYiUYaAQkTIMFCJShoFCRMoYfF0A+Zfc3FwcOHDArS0/Px8A8Prrr7u1Dx48GE888YTXaiP/p4mI+LoI8h9/+ctf8NRTT8FoNEKnq38D9s6dO3A6ndi1axdGjRrl5QrJnzFQyE1NTQ0iIiJw9erVRvuFhISguLgYBgM3cunveAyF3Oj1ekydOhUBAQEN9gkICMC0adMYJlQHA4XqmDx5Mqqqqhp8vqqqCpMnT/ZiRdRWcJeH6tWlSxecP3++3ueioqJw/vx5aJrm5arI33ELheqVmpoKo9FYpz0gIAAzZsxgmFC9uIVC9Tp+/DgSEhLqfe7rr79Gnz59vFwRtQUMFGpQQkICjh8/7tYWHx9fp42oFnd5qEHTp0932+0xGo2YMWOGDysif8ctFGrQ+fPn0bVrV9S+RTRNQ0FBAbp27erbwshvcQuFGhQTE4NHHnkEOp0OmqZh4MCBDBNqFAOFGjV9+nTodDro9XpMmzbN1+WQn+MuDzWqpKQE9913HwDgb3/7GyIiInxcEfmzdhco/P4EeVM7+3i1z8sXLFy4EI899pivy2gzcnNzoWkahg0bVu/z6enpAIAXXnjBm2X5tf379yMjI8PXZXhduwyUxx57DBMnTvR1GW3G6NGjAQBBQUH1Pr9t2zYA4DL9JwwUono0FCRE/4xneYhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISBkGChEpw0AhImUYKESkDAOFiJRhoDTT7Nmz0bFjR2iahq+++srX5bTInTt3kJ6ejsTERK+Om5OTg9jYWGia5vYICAhA586dMXz4cKxZswalpaVerYtajoHSTJs2bcI777zj6zJa7NSpUxg2bBgWLVoEh8Ph1bGTkpJQUFCAuLg42Gw2iAju3LmD4uJiZGVloVu3bkhLS0Pv3r3xxRdfeLU2ahkGSjt0+PBhvPTSS5g3bx4efvhhX5cD4LtLcwYHB2P48OHYsmULsrKycPnyZTzzzDMoKyvzdXnkIQbKXWjr16V96KGHkJOTg6lTpyIwMNDX5dQrOTkZM2fORHFxMd5++21fl0MeYqA0QUSwZs0a9OzZE4GBgbDZbFiyZEmdfjU1NVixYgViYmJgNpvRr18/2O12AMCGDRtgtVphsViwY8cOjBkzBkFBQYiKikJmZqbbdHJzczFo0CBYLBYEBQWhb9++KC8vb3KMe9HMmTMBAH/6059cbVzOfk7aGQBit9s97r9s2TLRNE3WrVsnpaWl4nA4ZP369QJA8vLyXP0WL14sgYGBkp2dLaWlpfLKK6+ITqeTgwcPuqYDQD766CMpKyuT4uJiGTp0qFitVqmqqhIRkZs3b0pQUJC8/vrrUllZKUVFRTJ+/HgpKSnxaIy78eijj8pDDz10168XEUlOTpbk5ORmvy4uLk5sNluDz5eXlwsAiY6OdrW1leVst9ulHX68pN3NcXMCxeFwiMVikVGjRrm1Z2ZmugVKZWWlWCwWSUlJcXttYGCgzJ8/X0T+/kavrKx09akNptOnT4uIyDfffCMA5MMPP6xTiydj3A1/DhQREU3TJDg4WETa1nJur4HCXZ5GnD59Gg6HAyNHjmy034kTJ+BwONCnTx9Xm9lsRmRkJPLz8xt8XUBAAADA6XQCAGJjY9G5c2ekpqZi5cqVOHv2bIvHaMsqKiogIq6LZHM5+z8GSiMuXrwIAAgPD2+0X0VFBQBg+fLlbt+rOHfuXLNOyZrNZuzevRtDhgzBqlWrEBsbi5SUFFRWVioboy05efIkACA+Ph4Al3NbwEBphMlkAgDcvn270X61gZOeng75bjfS9di/f3+zxuzduzd27tyJwsJCpKWlwW63Y+3atUrHaCv+/Oc/AwDGjBkDgMu5LWCgNKJPnz7Q6XTIzc1ttF90dDRMJlOLvzlbWFiIY8eOAfjuw/Paa6+hf//+OHbsmLIx2oqioiKkp6cjKioKP/zhDwFwObcFDJRGhIeHIykpCdnZ2di8eTPKy8tx5MgRbNy40a2fyWTCrFmzkJmZiQ0bNqC8vBw1NTW4ePEiLl265PF4hYWFmDt3LvLz81FVVYW8vDycO3cOgwcPVjaGvxER3Lx5E3fu3IGIoKSkBHa7HY8//jj0ej22b9/uOobC5dwGePkgsM+hmaeNb9y4IbNnz5bQ0FDp0KGDDBkyRFasWCEAJCoqSg4fPiwiIrdv35a0tDSJiYkRg8Eg4eHhkpSUJEePHpX169eLxWIRAPLggw/KmTNnZOPGjRIUFCQApEuXLnLy5Ek5e/asJCYmSkhIiOj1ern//vtl2bJlUl1d3eQYzbF//355/PHH5b777hMAAkAiIyMlMTFRcnNzmzUtkeaf5fnggw+kX79+YrFYJCAgQHQ6nQBwndEZNGiQ/OxnP5OrV6/WeW1bWc7t9SyPJtK+bg+vaRrsdjvvw6vQhAkTAPz9HscEZGVlYdKkSWhnHy/u8hCROgyUe0B+fn6dSwHU90hJSfF1qXSPM/i6AGq5+Pj4drdpTf6JWyhEpAwDhYiUYaAQkTIMFCJShoFCRMowUIhIGQYKESnDQCEiZRgoRKQMA4WIlGGgEJEyDBQiUoaBQkTKMFCISJl2ecU2Im9pZx+v9nc9FN6jlqj1tLstFCJqPTyGQkTKMFCISBkGChEpYwDAm6kQkRL/D6Y3XMJuc/s1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlTrcN1LNN8G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElF74gbMNSvb",
        "outputId": "be57fdbc-4ebb-43cc-d482-9c0bffdda702"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3STvsQJNWec",
        "outputId": "00f86c4a-85aa-4cd2-8e5b-dd48bd5a9a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8753\n",
            "Epoch 1: loss improved from inf to 7.87528, saving model to nextword1.h5\n",
            "61/61 [==============================] - 23s 287ms/step - loss: 7.8753 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8595\n",
            "Epoch 2: loss improved from 7.87528 to 7.85949, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 282ms/step - loss: 7.8595 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.8135\n",
            "Epoch 3: loss improved from 7.85949 to 7.81353, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 280ms/step - loss: 7.8135 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.6197\n",
            "Epoch 4: loss improved from 7.81353 to 7.61972, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 302ms/step - loss: 7.6197 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.4216\n",
            "Epoch 5: loss improved from 7.61972 to 7.42157, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 285ms/step - loss: 7.4216 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.2597\n",
            "Epoch 6: loss improved from 7.42157 to 7.25971, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 284ms/step - loss: 7.2597 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.1475\n",
            "Epoch 7: loss improved from 7.25971 to 7.14749, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 284ms/step - loss: 7.1475 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 7.0618\n",
            "Epoch 8: loss improved from 7.14749 to 7.06184, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 7.0618 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.9726\n",
            "Epoch 9: loss improved from 7.06184 to 6.97257, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 6.9726 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.8535\n",
            "Epoch 10: loss improved from 6.97257 to 6.85352, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 274ms/step - loss: 6.8535 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.6383\n",
            "Epoch 11: loss improved from 6.85352 to 6.63827, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 6.6383 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.3807\n",
            "Epoch 12: loss improved from 6.63827 to 6.38075, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 272ms/step - loss: 6.3807 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 6.1233\n",
            "Epoch 13: loss improved from 6.38075 to 6.12334, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 6.1233 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.8325\n",
            "Epoch 14: loss improved from 6.12334 to 5.83248, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 5.8325 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.5129\n",
            "Epoch 15: loss improved from 5.83248 to 5.51293, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 5.5129 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 5.2078\n",
            "Epoch 16: loss improved from 5.51293 to 5.20781, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 5.2078 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.8994\n",
            "Epoch 17: loss improved from 5.20781 to 4.89942, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 297ms/step - loss: 4.8994 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.6277\n",
            "Epoch 18: loss improved from 4.89942 to 4.62768, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 4.6277 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.3698\n",
            "Epoch 19: loss improved from 4.62768 to 4.36975, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 284ms/step - loss: 4.3698 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.1751\n",
            "Epoch 20: loss improved from 4.36975 to 4.17513, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 310ms/step - loss: 4.1751 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 4.0046\n",
            "Epoch 21: loss improved from 4.17513 to 4.00463, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 285ms/step - loss: 4.0046 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.7952\n",
            "Epoch 22: loss improved from 4.00463 to 3.79522, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 284ms/step - loss: 3.7952 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.6602\n",
            "Epoch 23: loss improved from 3.79522 to 3.66016, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 3.6602 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.5207\n",
            "Epoch 24: loss improved from 3.66016 to 3.52067, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 3.5207 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.4220\n",
            "Epoch 25: loss improved from 3.52067 to 3.42196, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 3.4220 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.2967\n",
            "Epoch 26: loss improved from 3.42196 to 3.29668, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 3.2967 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.1451\n",
            "Epoch 27: loss improved from 3.29668 to 3.14511, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 3.1451 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 3.0645\n",
            "Epoch 28: loss improved from 3.14511 to 3.06455, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 3.0645 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.9933\n",
            "Epoch 29: loss improved from 3.06455 to 2.99327, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 299ms/step - loss: 2.9933 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.9123\n",
            "Epoch 30: loss improved from 2.99327 to 2.91231, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 2.9123 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.8030\n",
            "Epoch 31: loss improved from 2.91231 to 2.80304, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 2.8030 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.7552\n",
            "Epoch 32: loss improved from 2.80304 to 2.75515, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 2.7552 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6785\n",
            "Epoch 33: loss improved from 2.75515 to 2.67847, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 2.6785 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.6294\n",
            "Epoch 34: loss improved from 2.67847 to 2.62942, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 2.6294 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.5556\n",
            "Epoch 35: loss improved from 2.62942 to 2.55561, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 2.5556 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4962\n",
            "Epoch 36: loss improved from 2.55561 to 2.49619, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 2.4962 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4589\n",
            "Epoch 37: loss improved from 2.49619 to 2.45893, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 272ms/step - loss: 2.4589 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.4327\n",
            "Epoch 38: loss improved from 2.45893 to 2.43267, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 2.4327 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3709\n",
            "Epoch 39: loss improved from 2.43267 to 2.37089, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 2.3709 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.3130\n",
            "Epoch 40: loss improved from 2.37089 to 2.31302, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 2.3130 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2596\n",
            "Epoch 41: loss improved from 2.31302 to 2.25964, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 2.2596 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.2189\n",
            "Epoch 42: loss improved from 2.25964 to 2.21885, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 296ms/step - loss: 2.2189 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1799\n",
            "Epoch 43: loss improved from 2.21885 to 2.17989, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 2.1799 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.1495\n",
            "Epoch 44: loss improved from 2.17989 to 2.14950, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 2.1495 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0992\n",
            "Epoch 45: loss improved from 2.14950 to 2.09925, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 2.0992 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0615\n",
            "Epoch 46: loss improved from 2.09925 to 2.06152, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 2.0615 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 2.0396\n",
            "Epoch 47: loss improved from 2.06152 to 2.03963, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 2.0396 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9921\n",
            "Epoch 48: loss improved from 2.03963 to 1.99211, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 282ms/step - loss: 1.9921 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9658\n",
            "Epoch 49: loss improved from 1.99211 to 1.96578, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 282ms/step - loss: 1.9658 - lr: 0.0010\n",
            "Epoch 50/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9285\n",
            "Epoch 50: loss improved from 1.96578 to 1.92848, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 1.9285 - lr: 0.0010\n",
            "Epoch 51/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.9091\n",
            "Epoch 51: loss improved from 1.92848 to 1.90914, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 1.9091 - lr: 0.0010\n",
            "Epoch 52/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8847\n",
            "Epoch 52: loss improved from 1.90914 to 1.88469, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 282ms/step - loss: 1.8847 - lr: 0.0010\n",
            "Epoch 53/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8819\n",
            "Epoch 53: loss improved from 1.88469 to 1.88188, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 282ms/step - loss: 1.8819 - lr: 0.0010\n",
            "Epoch 54/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8733\n",
            "Epoch 54: loss improved from 1.88188 to 1.87335, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 286ms/step - loss: 1.8733 - lr: 0.0010\n",
            "Epoch 55/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8273\n",
            "Epoch 55: loss improved from 1.87335 to 1.82730, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 313ms/step - loss: 1.8273 - lr: 0.0010\n",
            "Epoch 56/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8192\n",
            "Epoch 56: loss improved from 1.82730 to 1.81923, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 300ms/step - loss: 1.8192 - lr: 0.0010\n",
            "Epoch 57/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.8045\n",
            "Epoch 57: loss improved from 1.81923 to 1.80445, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 287ms/step - loss: 1.8045 - lr: 0.0010\n",
            "Epoch 58/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7615\n",
            "Epoch 58: loss improved from 1.80445 to 1.76146, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 287ms/step - loss: 1.7615 - lr: 0.0010\n",
            "Epoch 59/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7482\n",
            "Epoch 59: loss improved from 1.76146 to 1.74824, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 287ms/step - loss: 1.7482 - lr: 0.0010\n",
            "Epoch 60/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7067\n",
            "Epoch 60: loss improved from 1.74824 to 1.70668, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 1.7067 - lr: 0.0010\n",
            "Epoch 61/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6853\n",
            "Epoch 61: loss improved from 1.70668 to 1.68531, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 1.6853 - lr: 0.0010\n",
            "Epoch 62/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6493\n",
            "Epoch 62: loss improved from 1.68531 to 1.64926, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 280ms/step - loss: 1.6493 - lr: 0.0010\n",
            "Epoch 63/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6619\n",
            "Epoch 63: loss did not improve from 1.64926\n",
            "61/61 [==============================] - 17s 271ms/step - loss: 1.6619 - lr: 0.0010\n",
            "Epoch 64/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6442\n",
            "Epoch 64: loss improved from 1.64926 to 1.64415, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 1.6442 - lr: 0.0010\n",
            "Epoch 65/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.6343\n",
            "Epoch 65: loss improved from 1.64415 to 1.63429, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 1.6343 - lr: 0.0010\n",
            "Epoch 66/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5896\n",
            "Epoch 66: loss improved from 1.63429 to 1.58957, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 1.5896 - lr: 0.0010\n",
            "Epoch 67/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5797\n",
            "Epoch 67: loss improved from 1.58957 to 1.57971, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 300ms/step - loss: 1.5797 - lr: 0.0010\n",
            "Epoch 68/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5490\n",
            "Epoch 68: loss improved from 1.57971 to 1.54901, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 1.5490 - lr: 0.0010\n",
            "Epoch 69/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5139\n",
            "Epoch 69: loss improved from 1.54901 to 1.51387, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 1.5139 - lr: 0.0010\n",
            "Epoch 70/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5231\n",
            "Epoch 70: loss did not improve from 1.51387\n",
            "61/61 [==============================] - 16s 268ms/step - loss: 1.5231 - lr: 0.0010\n",
            "Epoch 71/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5347\n",
            "Epoch 71: loss did not improve from 1.51387\n",
            "61/61 [==============================] - 16s 266ms/step - loss: 1.5347 - lr: 0.0010\n",
            "Epoch 72/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.5493\n",
            "Epoch 72: loss did not improve from 1.51387\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "61/61 [==============================] - 16s 264ms/step - loss: 1.5493 - lr: 0.0010\n",
            "Epoch 73/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1737\n",
            "Epoch 73: loss improved from 1.51387 to 1.17367, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 1.1737 - lr: 2.0000e-04\n",
            "Epoch 74/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0006\n",
            "Epoch 74: loss improved from 1.17367 to 1.00065, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 1.0006 - lr: 2.0000e-04\n",
            "Epoch 75/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9364\n",
            "Epoch 75: loss improved from 1.00065 to 0.93637, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 0.9364 - lr: 2.0000e-04\n",
            "Epoch 76/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9060\n",
            "Epoch 76: loss improved from 0.93637 to 0.90600, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.9060 - lr: 2.0000e-04\n",
            "Epoch 77/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8853\n",
            "Epoch 77: loss improved from 0.90600 to 0.88527, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.8853 - lr: 2.0000e-04\n",
            "Epoch 78/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8727\n",
            "Epoch 78: loss improved from 0.88527 to 0.87265, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 0.8727 - lr: 2.0000e-04\n",
            "Epoch 79/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8595\n",
            "Epoch 79: loss improved from 0.87265 to 0.85952, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.8595 - lr: 2.0000e-04\n",
            "Epoch 80/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8507\n",
            "Epoch 80: loss improved from 0.85952 to 0.85072, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 292ms/step - loss: 0.8507 - lr: 2.0000e-04\n",
            "Epoch 81/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8459\n",
            "Epoch 81: loss improved from 0.85072 to 0.84592, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.8459 - lr: 2.0000e-04\n",
            "Epoch 82/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8387\n",
            "Epoch 82: loss improved from 0.84592 to 0.83873, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.8387 - lr: 2.0000e-04\n",
            "Epoch 83/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8365\n",
            "Epoch 83: loss improved from 0.83873 to 0.83645, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 0.8365 - lr: 2.0000e-04\n",
            "Epoch 84/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8290\n",
            "Epoch 84: loss improved from 0.83645 to 0.82899, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 0.8290 - lr: 2.0000e-04\n",
            "Epoch 85/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8266\n",
            "Epoch 85: loss improved from 0.82899 to 0.82661, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.8266 - lr: 2.0000e-04\n",
            "Epoch 86/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8193\n",
            "Epoch 86: loss improved from 0.82661 to 0.81935, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 0.8193 - lr: 2.0000e-04\n",
            "Epoch 87/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8200\n",
            "Epoch 87: loss did not improve from 0.81935\n",
            "61/61 [==============================] - 16s 268ms/step - loss: 0.8200 - lr: 2.0000e-04\n",
            "Epoch 88/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8181\n",
            "Epoch 88: loss improved from 0.81935 to 0.81806, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 0.8181 - lr: 2.0000e-04\n",
            "Epoch 89/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8104\n",
            "Epoch 89: loss improved from 0.81806 to 0.81037, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 0.8104 - lr: 2.0000e-04\n",
            "Epoch 90/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8093\n",
            "Epoch 90: loss improved from 0.81037 to 0.80929, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.8093 - lr: 2.0000e-04\n",
            "Epoch 91/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8063\n",
            "Epoch 91: loss improved from 0.80929 to 0.80625, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 303ms/step - loss: 0.8063 - lr: 2.0000e-04\n",
            "Epoch 92/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8070\n",
            "Epoch 92: loss did not improve from 0.80625\n",
            "61/61 [==============================] - 17s 286ms/step - loss: 0.8070 - lr: 2.0000e-04\n",
            "Epoch 93/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8032\n",
            "Epoch 93: loss improved from 0.80625 to 0.80319, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 282ms/step - loss: 0.8032 - lr: 2.0000e-04\n",
            "Epoch 94/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8019\n",
            "Epoch 94: loss improved from 0.80319 to 0.80188, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 0.8019 - lr: 2.0000e-04\n",
            "Epoch 95/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7977\n",
            "Epoch 95: loss improved from 0.80188 to 0.79772, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 275ms/step - loss: 0.7977 - lr: 2.0000e-04\n",
            "Epoch 96/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7954\n",
            "Epoch 96: loss improved from 0.79772 to 0.79535, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 274ms/step - loss: 0.7954 - lr: 2.0000e-04\n",
            "Epoch 97/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7939\n",
            "Epoch 97: loss improved from 0.79535 to 0.79393, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 297ms/step - loss: 0.7939 - lr: 2.0000e-04\n",
            "Epoch 98/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7912\n",
            "Epoch 98: loss improved from 0.79393 to 0.79125, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 0.7912 - lr: 2.0000e-04\n",
            "Epoch 99/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7898\n",
            "Epoch 99: loss improved from 0.79125 to 0.78980, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 0.7898 - lr: 2.0000e-04\n",
            "Epoch 100/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7892\n",
            "Epoch 100: loss improved from 0.78980 to 0.78922, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 0.7892 - lr: 2.0000e-04\n",
            "Epoch 101/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7874\n",
            "Epoch 101: loss improved from 0.78922 to 0.78742, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 0.7874 - lr: 2.0000e-04\n",
            "Epoch 102/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7847\n",
            "Epoch 102: loss improved from 0.78742 to 0.78473, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 0.7847 - lr: 2.0000e-04\n",
            "Epoch 103/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7820\n",
            "Epoch 103: loss improved from 0.78473 to 0.78196, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.7820 - lr: 2.0000e-04\n",
            "Epoch 104/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7806\n",
            "Epoch 104: loss improved from 0.78196 to 0.78064, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 285ms/step - loss: 0.7806 - lr: 2.0000e-04\n",
            "Epoch 105/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7773\n",
            "Epoch 105: loss improved from 0.78064 to 0.77728, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 299ms/step - loss: 0.7773 - lr: 2.0000e-04\n",
            "Epoch 106/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7771\n",
            "Epoch 106: loss improved from 0.77728 to 0.77709, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 0.7771 - lr: 2.0000e-04\n",
            "Epoch 107/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7754\n",
            "Epoch 107: loss improved from 0.77709 to 0.77536, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 282ms/step - loss: 0.7754 - lr: 2.0000e-04\n",
            "Epoch 108/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7766\n",
            "Epoch 108: loss did not improve from 0.77536\n",
            "61/61 [==============================] - 17s 273ms/step - loss: 0.7766 - lr: 2.0000e-04\n",
            "Epoch 109/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7727\n",
            "Epoch 109: loss improved from 0.77536 to 0.77269, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 282ms/step - loss: 0.7727 - lr: 2.0000e-04\n",
            "Epoch 110/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7719\n",
            "Epoch 110: loss improved from 0.77269 to 0.77193, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 0.7719 - lr: 2.0000e-04\n",
            "Epoch 111/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7718\n",
            "Epoch 111: loss improved from 0.77193 to 0.77181, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 288ms/step - loss: 0.7718 - lr: 2.0000e-04\n",
            "Epoch 112/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7682\n",
            "Epoch 112: loss improved from 0.77181 to 0.76822, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 287ms/step - loss: 0.7682 - lr: 2.0000e-04\n",
            "Epoch 113/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7710\n",
            "Epoch 113: loss did not improve from 0.76822\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 0.7710 - lr: 2.0000e-04\n",
            "Epoch 114/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7649\n",
            "Epoch 114: loss improved from 0.76822 to 0.76490, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 285ms/step - loss: 0.7649 - lr: 2.0000e-04\n",
            "Epoch 115/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7647\n",
            "Epoch 115: loss improved from 0.76490 to 0.76471, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 288ms/step - loss: 0.7647 - lr: 2.0000e-04\n",
            "Epoch 116/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7638\n",
            "Epoch 116: loss improved from 0.76471 to 0.76375, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 285ms/step - loss: 0.7638 - lr: 2.0000e-04\n",
            "Epoch 117/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7616\n",
            "Epoch 117: loss improved from 0.76375 to 0.76162, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 288ms/step - loss: 0.7616 - lr: 2.0000e-04\n",
            "Epoch 118/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7602\n",
            "Epoch 118: loss improved from 0.76162 to 0.76025, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 293ms/step - loss: 0.7602 - lr: 2.0000e-04\n",
            "Epoch 119/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7623\n",
            "Epoch 119: loss did not improve from 0.76025\n",
            "61/61 [==============================] - 17s 276ms/step - loss: 0.7623 - lr: 2.0000e-04\n",
            "Epoch 120/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7590\n",
            "Epoch 120: loss improved from 0.76025 to 0.75899, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 283ms/step - loss: 0.7590 - lr: 2.0000e-04\n",
            "Epoch 121/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7578\n",
            "Epoch 121: loss improved from 0.75899 to 0.75779, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 286ms/step - loss: 0.7578 - lr: 2.0000e-04\n",
            "Epoch 122/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7585\n",
            "Epoch 122: loss did not improve from 0.75779\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 0.7585 - lr: 2.0000e-04\n",
            "Epoch 123/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7583\n",
            "Epoch 123: loss did not improve from 0.75779\n",
            "61/61 [==============================] - 17s 274ms/step - loss: 0.7583 - lr: 2.0000e-04\n",
            "Epoch 124/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7574\n",
            "Epoch 124: loss improved from 0.75779 to 0.75740, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 283ms/step - loss: 0.7574 - lr: 2.0000e-04\n",
            "Epoch 125/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7536\n",
            "Epoch 125: loss improved from 0.75740 to 0.75357, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 284ms/step - loss: 0.7536 - lr: 2.0000e-04\n",
            "Epoch 126/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7524\n",
            "Epoch 126: loss improved from 0.75357 to 0.75235, saving model to nextword1.h5\n",
            "61/61 [==============================] - 20s 328ms/step - loss: 0.7524 - lr: 2.0000e-04\n",
            "Epoch 127/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7522\n",
            "Epoch 127: loss improved from 0.75235 to 0.75219, saving model to nextword1.h5\n",
            "61/61 [==============================] - 19s 306ms/step - loss: 0.7522 - lr: 2.0000e-04\n",
            "Epoch 128/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7510\n",
            "Epoch 128: loss improved from 0.75219 to 0.75096, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 277ms/step - loss: 0.7510 - lr: 2.0000e-04\n",
            "Epoch 129/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7510\n",
            "Epoch 129: loss did not improve from 0.75096\n",
            "61/61 [==============================] - 16s 262ms/step - loss: 0.7510 - lr: 2.0000e-04\n",
            "Epoch 130/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7457\n",
            "Epoch 130: loss improved from 0.75096 to 0.74567, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 290ms/step - loss: 0.7457 - lr: 2.0000e-04\n",
            "Epoch 131/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7468\n",
            "Epoch 131: loss did not improve from 0.74567\n",
            "61/61 [==============================] - 16s 262ms/step - loss: 0.7468 - lr: 2.0000e-04\n",
            "Epoch 132/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7475\n",
            "Epoch 132: loss did not improve from 0.74567\n",
            "61/61 [==============================] - 16s 263ms/step - loss: 0.7475 - lr: 2.0000e-04\n",
            "Epoch 133/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7451\n",
            "Epoch 133: loss improved from 0.74567 to 0.74510, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 273ms/step - loss: 0.7451 - lr: 2.0000e-04\n",
            "Epoch 134/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7470\n",
            "Epoch 134: loss did not improve from 0.74510\n",
            "61/61 [==============================] - 16s 262ms/step - loss: 0.7470 - lr: 2.0000e-04\n",
            "Epoch 135/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7469\n",
            "Epoch 135: loss did not improve from 0.74510\n",
            "61/61 [==============================] - 16s 265ms/step - loss: 0.7469 - lr: 2.0000e-04\n",
            "Epoch 136/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7478\n",
            "Epoch 136: loss did not improve from 0.74510\n",
            "\n",
            "Epoch 136: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "61/61 [==============================] - 16s 264ms/step - loss: 0.7478 - lr: 2.0000e-04\n",
            "Epoch 137/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6821\n",
            "Epoch 137: loss improved from 0.74510 to 0.68212, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 275ms/step - loss: 0.6821 - lr: 1.0000e-04\n",
            "Epoch 138/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6767\n",
            "Epoch 138: loss improved from 0.68212 to 0.67667, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 274ms/step - loss: 0.6767 - lr: 1.0000e-04\n",
            "Epoch 139/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6750\n",
            "Epoch 139: loss improved from 0.67667 to 0.67501, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 274ms/step - loss: 0.6750 - lr: 1.0000e-04\n",
            "Epoch 140/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6755\n",
            "Epoch 140: loss did not improve from 0.67501\n",
            "61/61 [==============================] - 16s 264ms/step - loss: 0.6755 - lr: 1.0000e-04\n",
            "Epoch 141/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6720\n",
            "Epoch 141: loss improved from 0.67501 to 0.67197, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 271ms/step - loss: 0.6720 - lr: 1.0000e-04\n",
            "Epoch 142/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6728\n",
            "Epoch 142: loss did not improve from 0.67197\n",
            "61/61 [==============================] - 17s 270ms/step - loss: 0.6728 - lr: 1.0000e-04\n",
            "Epoch 143/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6722\n",
            "Epoch 143: loss did not improve from 0.67197\n",
            "61/61 [==============================] - 16s 269ms/step - loss: 0.6722 - lr: 1.0000e-04\n",
            "Epoch 144/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6708\n",
            "Epoch 144: loss improved from 0.67197 to 0.67077, saving model to nextword1.h5\n",
            "61/61 [==============================] - 18s 291ms/step - loss: 0.6708 - lr: 1.0000e-04\n",
            "Epoch 145/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6718\n",
            "Epoch 145: loss did not improve from 0.67077\n",
            "61/61 [==============================] - 16s 265ms/step - loss: 0.6718 - lr: 1.0000e-04\n",
            "Epoch 146/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6719\n",
            "Epoch 146: loss did not improve from 0.67077\n",
            "61/61 [==============================] - 16s 264ms/step - loss: 0.6719 - lr: 1.0000e-04\n",
            "Epoch 147/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6708\n",
            "Epoch 147: loss did not improve from 0.67077\n",
            "61/61 [==============================] - 16s 260ms/step - loss: 0.6708 - lr: 1.0000e-04\n",
            "Epoch 148/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6705\n",
            "Epoch 148: loss improved from 0.67077 to 0.67047, saving model to nextword1.h5\n",
            "61/61 [==============================] - 17s 273ms/step - loss: 0.6705 - lr: 1.0000e-04\n",
            "Epoch 149/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6705\n",
            "Epoch 149: loss did not improve from 0.67047\n",
            "61/61 [==============================] - 16s 263ms/step - loss: 0.6705 - lr: 1.0000e-04\n",
            "Epoch 150/150\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6708\n",
            "Epoch 150: loss did not improve from 0.67047\n",
            "61/61 [==============================] - 16s 266ms/step - loss: 0.6708 - lr: 1.0000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f70d5fba350>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNSS1u44SrFm"
      },
      "outputs": [],
      "source": [
        "# Importing the Libraries\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "\n",
        "model = load_model('nextword1.h5')\n",
        "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "        In this function we are using the tokenizer and models trained\n",
        "        and we are creating the sequence of the text entered and then\n",
        "        using our model to predict and return the the predicted word.\n",
        "    \n",
        "    \"\"\"\n",
        "    for i in range(3):\n",
        "        \n",
        "        \n",
        "        sequence = tokenizer.texts_to_sequences([text])\n",
        "        #sequence = np.array(sequence)\n",
        "        \n",
        "        preds = model.predict(sequence)\n",
        "\n",
        "        predicted_word = \"\"\n",
        "        \n",
        "        \n",
        "        preds=np.argmax(preds)\n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            if value == preds:\n",
        "                predicted_word = key\n",
        "                break\n",
        "        \n",
        "        print(predicted_word)\n",
        "        return predicted_word"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QqxtlICPdsZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdkDztY1S8Ue",
        "outputId": "f469bfb7-eb74-46a0-d566-26ca00d11806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "less\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "still\n",
            "'pop from an empty set'\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    We are testing our model and we will run the model\n",
        "    until the user decides to stop the script.\n",
        "    While the script is running we try and check if \n",
        "    the prediction can be made on the text. If no\n",
        "    prediction can be made we just continue.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# text1 = \"at the dull\"\n",
        "# text2 = \"collection of textile\"\n",
        "# text3 = \"what a strenuous\"\n",
        "# text4 = \"stop the script\"\n",
        "\n",
        "while(True):\n",
        "\n",
        "    text = input(\"Enter your line: \")\n",
        "    \n",
        "    if text == \"stop the script\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    \n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-1]\n",
        "\n",
        "            text = ''.join(text)\n",
        "            Predict_Next_Words(model, tokenizer, text)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(e)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtm5r3R9ju3J8sqZwcoM6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}